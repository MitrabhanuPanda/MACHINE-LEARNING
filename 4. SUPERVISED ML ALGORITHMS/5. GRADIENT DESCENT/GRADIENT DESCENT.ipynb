{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5ea92f2f",
   "metadata": {},
   "source": [
    "<h1 style=\"text-align : center\"> <font color=\"red\" size=8>GRADIENT DESCENT</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c490c45a",
   "metadata": {},
   "source": [
    "## <font color=\"dark blue\">WHAT IS GRADIENT?\n",
    "- A gradient is nothing but a derivative that defines the effects on outputs of the function with a little bit of variation in inputs.\n",
    "\n",
    "- Gradient is a mathematical concept that measures the rate of change of a function.\n",
    "\n",
    "    - In machine learning, it's used to find the direction of steepest ascent or descent of a function.\n",
    "    \n",
    "    - This direction helps us adjust the model's parameters to minimize the error or loss.\n",
    "\n",
    "- eg:\n",
    "    - Think of a ball rolling down a hill. The gradient tells us the steepness of the hill at any point. The ball will roll in the direction of steepest descent to reach the bottom (the minimum point).\n",
    "        - The `hill` is the error or loss function.\n",
    "        - The `ball` is our model's parameters.\n",
    "        - The `gradient` tells us how to adjust the parameters to minimize the error. Â  \n",
    "        - By following the gradient, we can iteratively improve our model and find the optimal solution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "042306bc",
   "metadata": {},
   "source": [
    "## <font color=\"dark blue\">WHAT IS GRADIENT DESCENT?\n",
    "- Gradient descent is an optimization algorithm used to minimize the cost function in machine learning models.\n",
    "    \n",
    "\n",
    "- It iteratively adjusts the model parameters in the direction of the steepest descent of the cost function.\n",
    "    \n",
    "\n",
    "- It works by iteratively adjusting the weights or parameters of the model in the direction of the negative gradient of the cost function until the minimum of the cost function is reached.\n",
    "    \n",
    "\n",
    "- Gradient Descent is a fundamental optimization algorithm in machine learning used to minimize the cost or loss function during model training.\n",
    "    \n",
    "    - It iteratively adjusts model parameters by moving in the direction of the steepest decrease in the cost function.\n",
    "    \n",
    "    - The algorithm calculates gradients, representing the partial derivatives of the cost function concerning each parameter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "175c6cc3",
   "metadata": {},
   "source": [
    "## <font color=\"dark blue\">LEARNING RATE IN GRADIENT DESCENT\n",
    "- The learning rate is a critical hyperparameter in the context of gradient descent, influencing the size of steps taken during the optimization process to update the model parameters. Choosing an appropriate learning rate is crucial for efficient and effective model training.\n",
    "    \n",
    "\n",
    "- Achieving the right balance is essential. A small learning rate might result in vanishing gradients and slow convergence, while a large learning rate may lead to overshooting and instability."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2d20f52",
   "metadata": {},
   "source": [
    "## <font color=\"dark blue\">TYPES OF GRADIENT DESCENT \n",
    "### <font color=\"blue\">1. Batch Gradient Descent\n",
    "- In batch gradient descent, To update the model parameter values like weight and bias, the entire training dataset is used to compute the gradient and update the parameters at each iteration. \n",
    "- This can be slow for large datasets but may lead to a more accurate model. \n",
    "- It is effective for convex or relatively smooth error manifolds because it moves directly toward an optimal solution by taking a large step in the direction of the negative gradient of the cost function. \n",
    "- However, it can be slow for large datasets because it computes the gradient and updates the parameters using the entire training dataset at each iteration. \n",
    "- This can result in longer training times and higher computational costs.\n",
    "\n",
    "\n",
    "### <font color=\"blue\">2. Stochastic Gradient Descent (SGD)\n",
    "- In SGD, only one training example is used to compute the gradient and update the parameters at each iteration. \n",
    "- This can be faster than batch gradient descent but may lead to more noise in the updates.\n",
    "\n",
    "\n",
    "### <font color=\"blue\">3. Mini-batch Gradient Descent\n",
    "- In Mini-batch gradient descent a small batch of training examples is used to compute the gradient and update the parameters at each iteration. \n",
    "- This can be a good compromise between batch gradient descent and Stochastic Gradient Descent, as it can be faster than batch gradient descent and less noisy than Stochastic Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdd12bab",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43444e98",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c3c8f87",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4f5fccc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f3251aa4",
   "metadata": {},
   "source": [
    "## <font color=\"dark blue\">ADVANTAGES OF GRADIENT DESCENT\n",
    "- Flexibility\n",
    "- Scalability\n",
    "- Convergence\n",
    "\n",
    "\n",
    "## <font color=\"dark blue\">DISADVANTAGES OF GRADIENT DESCENT\n",
    "- Sensitive to learning rate\n",
    "- Slow convergence\n",
    "- Local Minima\n",
    "- Noisy Updates"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
