{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bea257b0",
   "metadata": {},
   "source": [
    "<h1 style=\"text-align : center\"> <font color=\"red\" size=8>REGULARIZATION </h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cd62a04",
   "metadata": {},
   "source": [
    "## <font color=\"dark blue\">WHAT IS REGULARIZATION?\n",
    "- Regularization introduces a penalty for more complex models, effectively reducing their complexity and encouraging the model to learn more generalized patterns\n",
    "\n",
    "- Regularization is a technique used to prevent overfitting by adding a penalty term to the loss function, discouraging the model from assigning too much importance to individual features or coefficients\n",
    "\n",
    "- It explicitly adds penalty term to the optimization problem.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23518648",
   "metadata": {},
   "source": [
    "## <font color=\"dark blue\">WHY WE USE REGULARIZATION?\n",
    "- Control Complexity of Model\n",
    "\n",
    "- Preventing Overfitting\n",
    "\n",
    "- Balancing Bias & Variance\n",
    "\n",
    "- Feature Selection\n",
    "\n",
    "- Handling Multicollinearity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b6d6c0b",
   "metadata": {},
   "source": [
    "## <font color=\"dark blue\">TYPES OF REGULARIZATION?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7f59da1",
   "metadata": {},
   "source": [
    "## <font color=\"blue\">1. LASSO (L1) REGRESSION\n",
    "- LASSO stands for __Least Absolute Shrinkage & Selection Operator__.\n",
    "\n",
    "- LASSO also know as __L1 Regularization__.\n",
    "\n",
    "- LASSO Regression adds the __`absolute value of magnitude`__ of the coefficient as a penalty term to the loss function(L).\n",
    "\n",
    "- LASSO regression also helps us achieve feature selection by penalizing the weights to approximately equal to zero if that feature does not serve any purpose in the model.\n",
    "\n",
    "$$ \\large Cost = \\frac{1}{n}\\sum_{i=1}^{n}(y_i - \\hat{y}_i)^2 \\ + \\lambda \\sum_{i=1}^{m} |w_i| $$\n",
    "    \n",
    "$$ where $$\n",
    "    \n",
    "$$ m \\rightarrow Number \\ of \\ Features $$\n",
    "$$ n \\rightarrow Number \\ of \\ Examples $$ \n",
    "$$ y_i \\rightarrow Actual \\ Target \\ Value $$\n",
    "$$ \\hat{y} \\rightarrow Predicted \\ \\ Target \\ Value $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d9072cc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "255f6674",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_diabetes\n",
    "\n",
    "data=load_diabetes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "77298554",
   "metadata": {},
   "outputs": [],
   "source": [
    "X=data.data\n",
    "y=data.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fddde4f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.2,random_state=45)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ed4f785b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression,Ridge,Lasso,ElasticNet\n",
    "L=LinearRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d708a2e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearRegression()"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "L.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "350de5b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred=L.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7719bf43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R2 score 0.5188118914964637\n",
      "RMSE 48.72710829141399\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import r2_score,mean_squared_error\n",
    "\n",
    "print(\"R2 score\",r2_score(y_test,y_pred))\n",
    "print(\"RMSE\",np.sqrt(mean_squared_error(y_test,y_pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "397941d2",
   "metadata": {},
   "source": [
    "### <font color=\"orange\"> CODE FOR LASSO REGRESSION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "32297aa5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5239820389650526"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Las = Lasso(alpha=0.01)\n",
    "Las.fit(X_train,y_train)\n",
    "y_pred = Las.predict(X_test)\n",
    "r2_score(y_test,y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "161fd5f0",
   "metadata": {},
   "source": [
    "## <font color=\"purple\"> PARAMETERS OF LASSO REGRESSION CLASS\n",
    "__1. alpha__:\n",
    "- This is the most important parameter. It controls the strength of the regularization.\n",
    "- A higher alpha value means stronger regularization, which can lead to more features being excluded from the model.\n",
    "\n",
    "\n",
    "__2. fit_intercept__:\n",
    "- Whether to calculate the intercept term in the regression equation.\n",
    "- Typically, you'll want to set this to `True`.\n",
    "\n",
    "\n",
    "__3. precompute__:\n",
    "- Whether to precompute the Gram matrix to speed up calculations. This can be useful for large datasets.\n",
    "\n",
    "\n",
    "__4. copy_X__:\n",
    "- Whether to copy the input data before fitting the model. This can be useful to avoid side effects.\n",
    "\n",
    "\n",
    "__5. max_iter__:\n",
    "- The maximum number of iterations for the optimization algorithm.\n",
    "\n",
    "\n",
    "__6. tol__:\n",
    "- Tolerance for the optimization algorithm.\n",
    "\n",
    "\n",
    "__7. warm_start__:\n",
    "- Whether to use the solution of the previous call to the fit method as the initial guess for the current call.\n",
    "\n",
    "\n",
    "__8. positive__:\n",
    "- Whether to force the coefficients to be positive.\n",
    "\n",
    "\n",
    "__9. random_state__:\n",
    "- The seed used by the random number generator.\n",
    "\n",
    "\n",
    "__10. selection__:\n",
    "- The algorithm used to select features during the optimization process."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82d54010",
   "metadata": {},
   "source": [
    "## <font color=\"blue\">2. RIDGE (L1) REGRESSION\n",
    "- Ridge also know as __L2 Regularization__.\n",
    "\n",
    "- Ridge Regression adds the __`squared of magnitude`__ of the coefficient as a penalty term to the loss function(L).\n",
    "\n",
    "- For every dimension, we square the slope/coefficient so that it is called __L2 Regularization__.\n",
    "\n",
    "$$ \\large Cost = \\frac{1}{n}\\sum_{i=1}^{n}(y_i - \\hat{y}_i)^2 \\ + \\lambda \\sum_{i=1}^{m} w_i^2 $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f10b7177",
   "metadata": {},
   "source": [
    "### <font color=\"orange\">CODE FOR RIDGE REGRESSION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f65304da",
   "metadata": {},
   "outputs": [],
   "source": [
    "R=Ridge(alpha=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d702f5d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Ridge(alpha=0.0001)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "R.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "638b6529",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred1=R.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "434d5f37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R2 score 0.5189738344370789\n",
      "RMSE 48.718908093712855\n"
     ]
    }
   ],
   "source": [
    "print(\"R2 score\",r2_score(y_test,y_pred1))\n",
    "print(\"RMSE\",np.sqrt(mean_squared_error(y_test,y_pred1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a1d4482",
   "metadata": {},
   "source": [
    "## <font color=\"purple\"> PARAMETERS OF RIDGE REGRESSION CLASS\n",
    "__1. alpha__:\n",
    "- This is the most important parameter. It controls the strength of the regularization.\n",
    "- A higher alpha value means stronger regularization, which can lead to smaller coefficient values.\n",
    "\n",
    "\n",
    "__2. fit_intercept__:\n",
    "- Whether to calculate the intercept term in the regression equation.\n",
    "- Typically, you'll want to set this to `True`.\n",
    "\n",
    "\n",
    "__3. copy_X__:\n",
    "- Whether to copy the input data before fitting the model. This can be useful to avoid side effects.\n",
    "\n",
    "\n",
    "__4. max_iter__:\n",
    "- The maximum number of iterations for the optimization algorithm.\n",
    "\n",
    "\n",
    "__5. tol__:\n",
    "- Tolerance for the optimization algorithm.\n",
    "\n",
    "\n",
    "__6. solver__:\n",
    "- The algorithm to use for optimization.\n",
    "- `auto` selects the best solver based on the data.\n",
    "\n",
    "\n",
    "__7. positive__:\n",
    "- Whether to force the coefficients to be positive.\n",
    "\n",
    "\n",
    "__8. random_state__:\n",
    "- The seed used by the random number generator."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcbb9121",
   "metadata": {},
   "source": [
    "## <font color=\"blue\">3. ELASTIC NET (L1 & L2) REGRESSION\n",
    "- It is the combination of both L1 & L2 Regularization.\n",
    "\n",
    "- With the help of an extra hyperparameter that controls the ratio of the L1 and L2 regularization\n",
    "\n",
    "- For every dimension, we square the slope/coeeficient so that it is called __L2 Regularization__.\n",
    "\n",
    "$$ \\large Cost = \\frac{1}{n}\\sum_{i=1}^{n}(y_i - \\hat{y}_i)^2 \\ + \\lambda ((1-\\alpha) \\sum_{i=1}^{m} |w_i| + \\alpha \\sum_{i=1}^{m}w_i^2) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f25e04f",
   "metadata": {},
   "source": [
    "### <font color=\"orange\">CODE FOR ELASTIC NET REGRESSION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ea16bfd2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5171423473020662"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Ela = ElasticNet(alpha=0.005,l1_ratio=0.9)\n",
    "Ela.fit(X_train,y_train)\n",
    "y_pred = Ela.predict(X_test)\n",
    "r2_score(y_test,y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed508a06",
   "metadata": {},
   "source": [
    "## <font color=\"purple\"> PARAMETERS OF ELASTIC NET REGRESSION CLASS\n",
    "__1. alpha__:\n",
    "- Controls the overall strength of regularization. A higher alpha means stronger regularization.\n",
    "\n",
    "\n",
    "__2. l1_ratio__:\n",
    "- Controls the balance between L1 and L2 regularization.\n",
    "    - If l1_ratio=0, it's equivalent to Ridge regression.\n",
    "    - If l1_ratio=1, it's equivalent to Lasso regression.\n",
    "    - Values between 0 and 1 give a combination of both.\n",
    "\n",
    "__3. fit_intercept__:\n",
    "- Whether to calculate the intercept term in the regression equation.\n",
    "- Typically, you'll want to set this to True.\n",
    "\n",
    "\n",
    "__4. precompute__:\n",
    "- Whether to precompute the Gram matrix to speed up calculations. This can be useful for large datasets.\n",
    "\n",
    "\n",
    "__5. max_iter__:\n",
    "- The maximum number of iterations for the optimization algorithm.\n",
    "\n",
    "\n",
    "__6. tol__:\n",
    "- Tolerance for the optimization algorithm.\n",
    "\n",
    "\n",
    "__7. warm_start__:\n",
    "- Whether to use the solution of the previous call to the fit method as the initial guess for the current call.\n",
    "\n",
    "\n",
    "__8. positive__:\n",
    "- Whether to force the coefficients to be positive.\n",
    "\n",
    "\n",
    "__9. random_state__:\n",
    "- The seed used by the random number generator.\n",
    "\n",
    "\n",
    "__10. selection__:\n",
    "- The algorithm used to select features during the optimization process."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c83e819d",
   "metadata": {},
   "source": [
    "## <font color=\"green\">Benefits of Regularization\n",
    "\n",
    "- Regularization improves model generalization by reducing overfitting. Regularized models learn underlying patterns, while overfit models memorize noise in training data.\n",
    "\n",
    "\n",
    "- Regularization techniques such as L1 (Lasso) L1 regularization simplifies models and improves interpretability by reducing coefficients of less important features to zero.\n",
    "\n",
    "\n",
    "- Regularization improves model performance by preventing excessive weighting of outliers or irrelevant features.\n",
    "\n",
    "\n",
    "- Regularization makes models stable across different subsets of the data. It reduces the sensitivity of model outputs to minor changes in the training set.\n",
    "\n",
    "\n",
    "- Regularization prevents models from becoming overly complex, which is especially important when dealing with limited data or noisy environments.\n",
    "\n",
    "\n",
    "- Regularization can help handle multicollinearity (high correlation between features) by reducing the magnitudes of correlated coefficients.\n",
    "\n",
    "\n",
    "- Regularization introduces hyperparameters (e.g., alpha or lambda) that control the strength of regularization. This allows fine-tuning models to achieve the right balance between bias and variance.\n",
    "\n",
    "\n",
    "- Regularization promotes consistent model performance across different datasets. It reduces the risk of dramatic performance changes when encountering new data"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
